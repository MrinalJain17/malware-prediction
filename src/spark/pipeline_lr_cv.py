from pyspark.sql import SparkSession

from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler, Normalizer
from pyspark.ml.classification import DecisionTreeClassifier, LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

HOME_DIR = 'hdfs://dumbo/user/ns4486/'
PIPELINE_DIR = HOME_DIR + 'pipelines/'
WRITE_DIR = HOME_DIR + 'write/'
READ_DIR = HOME_DIR + 'datasets/'
CV_DIR = HOME_DIR + 'cross_validators/'

spark = SparkSession.builder.getOrCreate()

train_df = spark.read.load(WRITE_DIR +'train_clean.parquet', format = "parquet", inferSchema = "true", header = "true")

learningModel = LogisticRegression(
    featuresCol = 'normFeatures',
    labelCol = 'HasDetections'
)

pipelineStages = []
pipelineStages += [learningModel]
pipeline = Pipeline(stages = pipelineStages)


paramGrid = ParamGridBuilder() \
    .addGrid(learningModel.threshold, [0.3, 0.4, 0.5, 0.6]) \
    .build()


evaluator = MulticlassClassificationEvaluator(
    labelCol = 'HasDetections',
    predictionCol = 'prediction',
    metricName = 'accuracy'
)

crossval = CrossValidator(
    estimator = pipeline,
    estimatorParamMaps = paramGrid,
    evaluator = evaluator,
    numFolds = 3
)

train_df = spark.createDataFrame(train_df.rdd, schema = train_df.schema)

cvModel = crossval.fit(train_df)

cvModel.save(CV_DIR + 'cv_lr')

# predictions = cvModel.transform(test_df)

# accuracy = evaluator.evaluate(predictions)

# print('[RESULTS] accuracy = ' + accuracy)