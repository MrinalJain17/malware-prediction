from pyspark.sql import SparkSession

from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler, Normalizer
from pyspark.ml.classification import DecisionTreeClassifier, LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

spark = SparkSession.builder.getOrCreate()

HOME_DIR = 'hdfs://dumbo/user/ns4486/'
PIPELINE_DIR = HOME_DIR + 'pipelines/'
WRITE_DIR = HOME_DIR + 'write/'

df = spark.read.load("datasets/microsoft_malware/train.csv", format = "csv", sep = ",", inferSchema = "true", header = "true")

# drop columns with NULL percentage > 50
df = df.drop(
    'DefaultBrowsersIdentifier',
    'PuaMode',
    'Census_ProcessorClass',
    'Census_InternalBatteryType',
    'Census_IsFlightingInternal',
    'Census_ThresholdOptIn',
    'Census_IsWIMBootEnabled'
)

# drop unique identifier column
df = df.drop(
    'MachineIdentifier'
)

df = df.fillna('NA')
train_df, test_df = df.randomSplit([0.75, 0.25], seed=1337)

COLUMNS_OHE = [
    'ProductName',
    'EngineVersion',
    'AppVersion',
    'IsBeta',
    'RtpStateBitfield',
    'IsSxsPassiveMode',
    'AVProductsInstalled',
    'AVProductsEnabled',
    'HasTpm',
    'CountryIdentifier',
    'OrganizationIdentifier',
    'GeoNameIdentifier',
    'LocaleEnglishNameIdentifier',
    'Platform',
    'Processor',
    'OsVer',
    'OsBuild',
    'OsSuite',
    'OsPlatformSubRelease',
    'OsBuildLab',
    'SkuEdition',
    'IsProtected',
    'AutoSampleOptIn',
    'SMode',
    'IeVerIdentifier',
    'SmartScreen',
    'Firewall',
    'UacLuaenable',
    'Census_MDC2FormFactor',
    'Census_DeviceFamily',
    'Census_ProcessorCoreCount',
    'Census_ProcessorManufacturerIdentifier',
    'Census_PrimaryDiskTypeName',
    'Census_HasOpticalDiskDrive',
    'Census_ChassisTypeName',
    'Census_InternalPrimaryDiagonalDisplaySizeInInches',
    'Census_PowerPlatformRoleName',
    'Census_OSVersion',
    'Census_OSArchitecture',
    'Census_OSBranch',
    'Census_OSBuildNumber',
    'Census_OSBuildRevision',
    'Census_OSEdition',
    'Census_OSSkuName',
    'Census_OSInstallTypeName',
    'Census_OSInstallLanguageIdentifier',
    'Census_OSUILocaleIdentifier',
    'Census_OSWUAutoUpdateOptionsName',
    'Census_IsPortableOperatingSystem',
    'Census_GenuineStateName',
    'Census_ActivationChannel',
    'Census_IsFlightsDisabled',
    'Census_FlightRing',
    'Census_FirmwareManufacturerIdentifier',
    'Census_IsSecureBootEnabled',
    'Census_IsVirtualDevice',
    'Census_IsTouchEnabled',
    'Census_IsPenCapable',
    'Census_IsAlwaysOnAlwaysConnectedCapable',
    'Wdft_IsGamer',
    'Wdft_RegionIdentifier'
]

COLUMNS_HIGH_CARD = [
    'Census_SystemVolumeTotalCapacity',
    'Census_PrimaryDiskTotalCapacity',
    'Census_TotalPhysicalRAM',
    'Census_OEMModelIdentifier',
    'CityIdentifier',
    'Census_FirmwareVersionIdentifier',
    'Census_InternalBatteryNumberOfCharges',
    'AVProductStatesIdentifier',
    'AvSigVersion',
    'Census_OEMNameIdentifier',
    'Census_ProcessorModelIdentifier',
    'Census_InternalPrimaryDisplayResolutionHorizontal',
    'Census_InternalPrimaryDisplayResolutionVertical'
]

pipelineStages = []

stringIndexerStages = [
    StringIndexer(inputCol = col, outputCol = col + '_INDEX', handleInvalid = 'skip')
    for col in COLUMNS_OHE + COLUMNS_HIGH_CARD
]
pipelineStages += stringIndexerStages

OHEStage = OneHotEncoderEstimator(
    inputCols = [col + '_INDEX' for col in COLUMNS_OHE],
    outputCols = [col + '_VEC' for col in COLUMNS_OHE]
)
pipelineStages += [OHEStage]

sparseVectorCols = [col + '_VEC' for col in COLUMNS_OHE] + [col + '_INDEX' for col in COLUMNS_HIGH_CARD]
assembler = VectorAssembler(
    inputCols = sparseVectorCols, 
    outputCol = 'features'
)
pipelineStages += [assembler]

normalizer = Normalizer(
    inputCol = 'features',
    outputCol = 'normFeatures'
)
pipelineStages += [normalizer]

train_df = spark.createDataFrame(train_df.rdd, schema = train_df.schema)

pipeline = Pipeline(stages = pipelineStages)
pipelineModel = pipeline.fit(train_df)
train_df = pipelineModel.transform(train_df)

pipelineModel.save(PIPELINE_DIR + 'pipeline_model_preprocess')
pipeline.save(PIPELINE_DIR + 'pipeline_preprocess')

train_df.write.parquet(WRITE_DIR + 'train_clean.parquet')